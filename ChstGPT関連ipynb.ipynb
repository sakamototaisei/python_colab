{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNd7X5Oxo1SAHZICNv7EusQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakamototaisei/python_colab/blob/main/ChstGPT%E9%96%A2%E9%80%A3ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **セクション1：イントロ**"
      ],
      "metadata": {
        "id": "gXtuGKS2wic1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **OpenAIの簡単な歴史**"
      ],
      "metadata": {
        "id": "yf7igo1C0zme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   2015年に非営利団体として設立された→現在は営利団体企業となり大きな転換をする\n",
        "*   最先端の機械学習やAIの研究を行っている、GPT-4やDALL-E、Whisperのような非常に複雑なモデルを構築してる、フレームワークやAPIも開発\n",
        "*   2018年12月OpenAIは大規模言語モデルであるGPT-2を発表\n",
        "*   2019年営利企業へと変化する\n",
        "*   2020年6月にGPT-3をリリースする\n",
        "*   2020年12月にDALL-Eをリリース、画像生成モデル\n",
        "*   2022年11月にChat-GPTをリリース\n",
        "*   2023年3月にはChat APIを公開した\n",
        "\n"
      ],
      "metadata": {
        "id": "moEioPmS06Z1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **GPTについて**"
      ],
      "metadata": {
        "id": "PLOEg5Nu3yUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   テキストを渡してテキストを出力として得ることができる\n",
        "*   次の単語を予測するように学習されている"
      ],
      "metadata": {
        "id": "jsQUsEVY32E3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transoformerアーキテクチャ**"
      ],
      "metadata": {
        "id": "LEdAlOHZ5i9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ChatGPT4に聞いてみた**\n",
        "\n",
        "**Transformerアーキテクチャとは？**\n",
        "\n",
        "Transformerは、自然言語処理のタスク、特に文章の翻訳や文章の生成などにおいて非常に優れた性能を示すニューラルネットワークの一種です。このアーキテクチャは「Attention Is All You Need」という論文で初めて紹介されました。名前が示す通り、「Attention」という概念が中心になっています。\n",
        "\n",
        "**Attention メカニズム**\n",
        "\n",
        "Attentionは、入力データのどの部分に注目すべきかをモデルに教える役割を持っています。たとえば、英語の文をフランス語に翻訳する際、Transformerは特定のフランス語の単語を生成するのに、英語の文のどの部分が重要かを判断します。\n",
        "\n",
        "**Transformerの主要な要素**\n",
        "\n",
        "1.  Encoder: Encoderは、入力データ（例：英語の文）をベクトルの形式で表現します。これにより、モデルが入力データの意味や構造を理解できるようになります。\n",
        "\n",
        "2.  Decoder: Decoderは、Encoderの出力を受け取り、ターゲットデータ（例：フランス語の文）を生成します。\n",
        "\n",
        "**どのように動作するのか？**\n",
        "\n",
        "1. Input Embedding: 入力文は単語ごとにベクトルに変換されます。これを埋め込みベクトルと呼びます。\n",
        "\n",
        "2. Positional Encoding: Transformerは、系列データの順序を理解する必要がありますが、自身には順序を表す情報がないため、位置情報を追加します。これがPositional Encodingです。\n",
        "\n",
        "3. Encoder Layers: これらのベクトルはEncoderに入ります。Encoderは複数の層で構成され、各層はSelf-AttentionとFeed Forward Neural Networkからなります。\n",
        "\n",
        "4. Decoder Layers: Encoderの出力はDecoderに送られます。Decoderも複数の層で構成されており、Encoderからの出力に注目しながら新しい単語を生成します。\n",
        "\n",
        "5. Output Linear Layer: Decoderの最後の層は、ターゲット言語の単語を生成します。\n",
        "\n",
        "**なぜTransformerは重要か？**\n",
        "\n",
        "Transformerアーキテクチャは並列処理に優れており、RNN(Recurrent Neural Networks) や LSTM (Long Short-Term Memory) と比べて、大規模なデータセットで高速に学習することができます。これにより、より大規模で複雑なNLPタスクに取り組むことができるようになりました。\n",
        "\n",
        "また、Attentionメカニズムによって、Transformerは系列データの中での関連性や依存関係を捉える能力が非常に高くなります。これにより、文の翻訳や文章生成において文脈や意味を考慮した結果を出力することができます。\n",
        "\n",
        "**Transformerを利用したモデル**\n",
        "\n",
        "Transformerアーキテクチャは非常に影響力があり、その後の多くのモデルの基盤となりました。代表的なものには以下のようなものがあります。\n",
        "\n",
        "* BERT (Bidirectional Encoder Representations from Transformers): BERTはTransformerのEncoderのみを使用し、テキストデータを深く理解することに焦点を当てています。BERTはテキストの分類、名前付きエンティティ認識、質問応答など、多くのNLPタスクで高い性能を発揮します。\n",
        "\n",
        "* GPT (Generative Pre-trained Transformer): GPTはTransformerのDecoder部分を使用して、テキストを生成することに特化しています。GPTは文章生成、文章の補完、文章の要約などのタスクで広く使用されています。\n",
        "\n",
        "**まとめ**\n",
        "\n",
        "Transformerアーキテクチャは、Attentionメカニズムを中心に、大規模なテキストデータを効率的に処理し、深い文脈理解を実現するためのニューラルネットワークです。これにより、NLPの分野における多くの進歩がもたらされ、BERTやGPTのような派生モデルも開発されました。これらのモデルは、自然言語処理の分野で広く使用され、高い性能を発揮しています。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TnHrIJCf5rw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3912u2KdIbQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **セクション2：始めよう**"
      ],
      "metadata": {
        "id": "MDxrfUKtIhEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **初めてのCompletionリクエスト**"
      ],
      "metadata": {
        "id": "Qbd-aaOHNXmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VKp49np30zWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iNDU3uk000le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5hSfPT5F00jI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zNNWpB_Y00gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tcAq3WSC00eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qWMR34ju00cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K0LOX-mW00Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O7nKWr8000XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SRBeVqEf00Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JBF4YYYY00SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ow6s1tdU00P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ijv4xTFK00Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IdKo17LR00LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vTHEUBWf00JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6lCsBqjg00Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-kcwqKAJ00EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2IRYaeLwK-m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LEK2bwadwbm4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}